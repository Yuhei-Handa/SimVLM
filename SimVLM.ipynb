{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class SimVLM(nn.Module):\n",
    "    def __init__(self, batch_size, num_patch, num_embedding, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # モデルのパラメータを初期化\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.num_heads = num_heads\n",
    "        self.ffn_hidden_size = ffn_hidden_size\n",
    "        self.num_patch = num_patch\n",
    "\n",
    "        # エンコーダーおよびデコーダーの埋め込み層を定義\n",
    "        self.encoder_embedding = nn.Embedding(num_embedding, hidden_size, padding_idx=0)\n",
    "        self.decoder_embedding = nn.Embedding(num_embedding, hidden_size, padding_idx=0)\n",
    "\n",
    "        # 位置エンベディングの初期化\n",
    "        self._setupPositionalEmbedding(num_patch, seq_length, hidden_size)\n",
    "\n",
    "        # ResNetモデルの初期化\n",
    "        self.resnet = ResNet(num_patch, hidden_size)\n",
    "\n",
    "        # エンコーダーとデコーダーの初期化\n",
    "        self.encoder = Encoder(batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n",
    "        self.decoder = Decoder(batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n",
    "\n",
    "    def _setupPositionalEmbedding(self, num_patch, seq_length, hidden_size):\n",
    "        # 位置エンベディングを初期化するメソッド\n",
    "        image_positional_embedding_module = nn.Embedding(num_patch, hidden_size)\n",
    "        encoder_positional_embedding_module = nn.Embedding(seq_length[0], hidden_size)\n",
    "        decoder_positional_embedding_module = nn.Embedding(seq_length[1], hidden_size)\n",
    "\n",
    "        image_position_ids = torch.tensor(list(range(num_patch))).expand(self.batch_size, -1)\n",
    "        encoder_position_ids = torch.tensor(list(range(seq_length[0]))).expand(self.batch_size, -1)\n",
    "        decoder_position_ids = torch.tensor(list(range(seq_length[1]))).expand(self.batch_size, -1)\n",
    "\n",
    "        # 画像、エンコーダー、デコーダーそれぞれの位置エンベディングを初期化\n",
    "        self.image_positional_embs = image_positional_embedding_module(image_position_ids)\n",
    "        self.encoder_positional_embs = encoder_positional_embedding_module(encoder_position_ids)\n",
    "        self.decoder_positional_embs = decoder_positional_embedding_module(decoder_position_ids)\n",
    "\n",
    "    def forward(self, images, encoder_input_ids, decoder_input_ids):\n",
    "        # モデルのフォワードパスを定義\n",
    "\n",
    "        # 画像からエンコーダーへの処理\n",
    "        encoder_image_output = self.resnet(images) + self.image_positional_embs\n",
    "\n",
    "        # エンコーダーのトークン埋め込みと位置エンベディングの結合\n",
    "        encoder_tokens_embedding = self.encoder_embedding(encoder_input_ids) + self.encoder_positional_embs\n",
    "        encoder_concat_tokens = torch.cat([encoder_image_output, encoder_tokens_embedding], dim=1)\n",
    "\n",
    "        # エンコーダーの処理\n",
    "        encoder_output = self.encoder(encoder_concat_tokens)\n",
    "\n",
    "        # デコーダーのトークン埋め込みの取得\n",
    "        decoder_embedding = self.decoder_embedding(decoder_input_ids)\n",
    "\n",
    "        # デコーダーの処理\n",
    "        decoder_output = self.decoder(decoder_embedding, encoder_output)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_patch, output_size):\n",
    "        super().__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self._initModel(output_size)\n",
    "\n",
    "    def _initModel(self, output_size):\n",
    "        # 事前学習済みのResNet-18モデルをロード\n",
    "        model = resnet18(pretrained=True)\n",
    "        in_features = 1\n",
    "\n",
    "        # 入力チャネル数を調整\n",
    "        conv1_in_channels = self.num_patch\n",
    "        conv1_out_channels = model.conv1.out_channels\n",
    "        model.conv1 = nn.Conv2d(conv1_in_channels, conv1_out_channels, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "        # 逆畳み込み層の入力チャネル数を調整\n",
    "        deconv_in_channels = model.layer3[1].conv2.out_channels\n",
    "        deconv = nn.Conv2d(deconv_in_channels, self.num_patch, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "\n",
    "        # 線形層を設定\n",
    "        self.fc = nn.Linear(in_features, output_size)\n",
    "\n",
    "        # モデルの構築\n",
    "        my_model = nn.Sequential(model.conv1, model.bn1, model.relu, model.maxpool, model.layer1, model.layer2, model.layer3, deconv, model.avgpool)\n",
    "        self.model = my_model\n",
    "\n",
    "    def patchImages(self, images):\n",
    "        # 画像をトークンに変換する関数\n",
    "        batch_size, num_channel, width, height = images.shape\n",
    "        patch_size = int(width / ((self.num_patch / num_channel) ** 0.5))\n",
    "        patch_window = torch.ones((patch_size, patch_size), dtype=torch.long)\n",
    "\n",
    "        # パッチウィンドウを画像のチャネルに拡張\n",
    "        patch_window = patch_window.unsqueeze(0).expand(num_channel, patch_size, patch_size) \\\n",
    "            .unsqueeze(0).expand(batch_size, num_channel, patch_size, patch_size)\n",
    "\n",
    "        token_list = []\n",
    "\n",
    "        # 画像をパッチに分割\n",
    "        for row_idx in range(0, width, patch_size):\n",
    "            for col_idx in range(0, height, patch_size):\n",
    "                patch = images[:, :, row_idx: row_idx + patch_size, col_idx: col_idx + patch_size]\n",
    "                token_list.append(patch)\n",
    "\n",
    "        # パッチをスタックし、形状を整える\n",
    "        patched_images = torch.stack(token_list, dim=0).transpose(0, 1).reshape(batch_size, self.num_patch, patch_size, patch_size)\n",
    "\n",
    "        return patched_images\n",
    "    \n",
    "    def forward(self, images):\n",
    "        patched_images = self.patchImages(images)\n",
    "        image_tokens = self.model(patched_images)\n",
    "        image_tokens = image_tokens.reshape(-1, self.num_patch, 1)\n",
    "        image_tokens = self.fc(image_tokens)\n",
    "\n",
    "        return image_tokens\n",
    "        \n",
    "    \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, batch_size, num_patch, num_heads, seq_length, hidden_size, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-Head Attention レイヤー\n",
    "        self.multi_head_attention = MultiHeadAttention(batch_size, num_patch, num_heads, seq_length, hidden_size, check_positional_embedding=True, check_mask=False)\n",
    "\n",
    "        # Add & Norm レイヤー1\n",
    "        self.add_norm1 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=True)\n",
    "\n",
    "        # FeedForward レイヤー\n",
    "        self.feed_forward = FeedForward(hidden_size, ffn_hidden_size)\n",
    "\n",
    "        # Add & Norm レイヤー2\n",
    "        self.add_norm2 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=True)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # 入力トークンを保持しておく（Skip Connection用）\n",
    "        skip1 = tokens\n",
    "\n",
    "        # Multi-Head Attention レイヤーの処理\n",
    "        multi_head_attention = self.multi_head_attention(tokens, tokens, tokens)\n",
    "\n",
    "        # Add & Norm レイヤー1の処理\n",
    "        add_norm1 = self.add_norm1(multi_head_attention, skip1)\n",
    "\n",
    "        # Skip Connectionを保持しておく\n",
    "        skip2 = add_norm1\n",
    "\n",
    "        # FeedForward レイヤーの処理\n",
    "        feed_forward = self.feed_forward(add_norm1)\n",
    "\n",
    "        # Add & Norm レイヤー2の処理\n",
    "        add_norm2 = self.add_norm2(feed_forward, skip2)\n",
    "\n",
    "        # 処理結果を返す\n",
    "        tokens = add_norm2\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # 複数のエンコーダーレイヤーを構築\n",
    "        self._setupEncoderLayer(batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n",
    "\n",
    "    def _setupEncoderLayer(self, batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        # エンコーダーレイヤーをリストとして保持するためのモジュール\n",
    "        encoder_layer_list = []\n",
    "\n",
    "        # 指定された数だけエンコーダーレイヤーを構築\n",
    "        for _ in range(num_layer):\n",
    "            encoder_layer = EncoderLayer(batch_size, num_patch, num_heads, seq_length, hidden_size, ffn_hidden_size)\n",
    "            encoder_layer_list.append(encoder_layer)\n",
    "\n",
    "        # モジュールリストとしてエンコーダーレイヤーを保持\n",
    "        self.encoder_module = nn.ModuleList(encoder_layer_list)\n",
    "\n",
    "    def forward(self, encoder_embedding):\n",
    "        # 入力トークンを保持しておく\n",
    "        tokens = encoder_embedding\n",
    "\n",
    "        # 各エンコーダーレイヤーを順次適用\n",
    "        for encoder_layer in self.encoder_module:\n",
    "            tokens = encoder_layer(tokens)\n",
    "\n",
    "        # エンコーダーの出力を返す\n",
    "        encoder_output = tokens\n",
    "\n",
    "        return encoder_output\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, batch_size, num_patch, num_heads, seq_length, hidden_size, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # マスク付きのMulti-Head Attention レイヤー\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(batch_size, num_patch, num_heads, seq_length, hidden_size, check_positional_embedding=False, check_mask=True)\n",
    "\n",
    "        # Add & Norm レイヤー1\n",
    "        self.add_norm1 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=False)\n",
    "\n",
    "        # Cross-Attention レイヤー\n",
    "        self.cross_multi_head_attention = MultiHeadAttention(batch_size, num_patch, num_heads, seq_length, hidden_size, check_positional_embedding=False, check_mask=False)\n",
    "\n",
    "        # Add & Norm レイヤー2\n",
    "        self.add_norm2 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=False)\n",
    "\n",
    "        # FeedForward レイヤー\n",
    "        self.feed_forward = FeedForward(hidden_size, ffn_hidden_size)\n",
    "\n",
    "        # Add & Norm レイヤー3\n",
    "        self.add_norm3 = AddNorm(batch_size, num_patch, seq_length, hidden_size, check_encoder=False)\n",
    "\n",
    "    def forward(self, tokens, output_encoder):\n",
    "        # 入力トークンを保持しておく\n",
    "        skip1 = tokens\n",
    "\n",
    "        # マスク付きのMulti-Head Attention レイヤーの処理\n",
    "        masked_multi_head_attention = self.masked_multi_head_attention(tokens, tokens, tokens)\n",
    "\n",
    "        # Add & Norm レイヤー1の処理\n",
    "        add_norm1 = self.add_norm1(masked_multi_head_attention, skip1)\n",
    "\n",
    "        # Skip Connectionを保持しておく\n",
    "        skip2 = add_norm1\n",
    "\n",
    "        # Cross-Attention レイヤーの処理\n",
    "        cross_multi_head_attention = self.cross_multi_head_attention(tokens, output_encoder, output_encoder)\n",
    "\n",
    "        # Add & Norm レイヤー2の処理\n",
    "        add_norm2 = self.add_norm2(cross_multi_head_attention, skip2)\n",
    "\n",
    "        # Skip Connectionを保持しておく\n",
    "        skip3 = add_norm2\n",
    "\n",
    "        # FeedForward レイヤーの処理\n",
    "        feed_forward = self.feed_forward(tokens)\n",
    "\n",
    "        # Add & Norm レイヤー3の処理\n",
    "        add_norm3 = self.add_norm3(feed_forward, skip3)\n",
    "\n",
    "        # 処理結果を返す\n",
    "        tokens = add_norm3\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # 複数のデコーダーレイヤーを構築\n",
    "        self._setupDecoderLayer(batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size)\n",
    "\n",
    "    def _setupDecoderLayer(self, batch_size, num_patch, seq_length, hidden_size, num_layer, num_heads, ffn_hidden_size):\n",
    "        # デコーダーレイヤーをリストとして保持するためのモジュール\n",
    "        decoder_layer_list = []\n",
    "\n",
    "        # 指定された数だけデコーダーレイヤーを構築\n",
    "        for _ in range(num_layer):\n",
    "            decoder_layer = DecoderLayer(batch_size, num_patch, num_heads, seq_length, hidden_size, ffn_hidden_size)\n",
    "            decoder_layer_list.append(decoder_layer)\n",
    "\n",
    "        # モジュールリストとしてデコーダーレイヤーを保持\n",
    "        self.decoder_module = nn.ModuleList(decoder_layer_list)\n",
    "\n",
    "    def forward(self, decoder_embedding, encoder_output):\n",
    "        # 入力トークンを保持しておく\n",
    "        tokens = decoder_embedding\n",
    "\n",
    "        # 各デコーダーレイヤーを順次適用\n",
    "        for decoder_layer in self.decoder_module:\n",
    "            tokens = decoder_layer(tokens, encoder_output)\n",
    "\n",
    "        # デコーダーの出力を返す\n",
    "        decoder_output = tokens\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, batch_size, num_patch, num_heads, seq_length, hidden_size, check_positional_embedding, check_mask):\n",
    "        super().__init__()\n",
    "\n",
    "        # Query、Key、Value用の線形変換モジュールを構築\n",
    "        self._setupHeadQKV(num_heads, hidden_size)\n",
    "\n",
    "        # モジュールのパラメータや設定を保持\n",
    "        self.batch_size = batch_size\n",
    "        self.num_patch = num_patch\n",
    "        self.num_heads = num_heads\n",
    "        self.seq_length = seq_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.check_positional_embedding = check_positional_embedding\n",
    "        self.check_mask = check_mask\n",
    "\n",
    "        # Softmax関数（Attentionの重み計算用）\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def _setupHeadQKV(self, num_heads, hidden_size):\n",
    "        # Query、Key、Value用のモジュールをリストとして保持するためのモジュール\n",
    "        query_module = []\n",
    "        key_module = []\n",
    "        value_module = []\n",
    "\n",
    "        # ヘッドごとの隠れ層サイズ\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "\n",
    "        # 指定されたヘッド数だけQuery、Key、Value用のモジュールを構築\n",
    "        for _ in range(num_heads):\n",
    "            query_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "            key_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "            value_module.append(nn.Linear(hidden_size, head_hidden_size))\n",
    "\n",
    "        # モジュールリストとして保持\n",
    "        self.query_module = nn.ModuleList(query_module)\n",
    "        self.key_module = nn.ModuleList(key_module)\n",
    "        self.value_module = nn.ModuleList(value_module)\n",
    "\n",
    "    def _outputRelativePositionalEmbeddingScalar(self, query, batch_size, num_patch, seq_length, hidden_size, num_heads):\n",
    "        # 相対位置エンベディングを計算する関数\n",
    "\n",
    "        # 入力のシーケンス長を取得\n",
    "        seq_length = seq_length[0]\n",
    "\n",
    "        # Embeddingモジュールを保持するためのリスト\n",
    "        embed_Module = []\n",
    "\n",
    "        # ヘッドごとの隠れ層サイズ\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "\n",
    "        # 位置情報のIDを作成\n",
    "        position_ids = torch.tensor(list(range(num_patch + seq_length)), dtype=torch.long).reshape(1, num_patch + seq_length).expand(batch_size, num_patch + seq_length)\n",
    "\n",
    "        # 各ヘッドごとにEmbeddingモジュールを構築\n",
    "        for id in range(num_heads):\n",
    "            embed_Module.append(nn.Embedding(num_patch + seq_length, head_hidden_size))\n",
    "\n",
    "        # モジュールリストとしてEmbeddingモジュールを保持\n",
    "        self.embed_module = nn.ModuleList(embed_Module)\n",
    "\n",
    "        # 各ヘッドごとに相対位置エンベディングを計算\n",
    "        for id in range(num_heads):\n",
    "            head_query = self.query_module[id](query)\n",
    "            tmp_relative_position_embedding_scalar = (head_query@(self.embed_module[id](position_ids).transpose(1, 2)))\\\n",
    "                .reshape(1, batch_size, num_patch + seq_length, num_patch + seq_length)[:, :, :num_patch, :num_patch]\n",
    "            \n",
    "            # 列のパディングを追加\n",
    "            col_pad = torch.zeros((1, batch_size, num_patch, seq_length), dtype=torch.float)\n",
    "            tmp_relative_position_embedding_scalar = torch.cat([tmp_relative_position_embedding_scalar, col_pad], dim=3)\n",
    "            \n",
    "            # 行のパディングを追加\n",
    "            row_pad = torch.zeros((1, batch_size, seq_length, num_patch + seq_length))\n",
    "            tmp_relative_position_embedding_scalar = torch.cat([tmp_relative_position_embedding_scalar, row_pad], dim=2)\n",
    "            \n",
    "            # 初めて計算するヘッドの場合は、相対位置エンベディングをそのまま保持\n",
    "            if id == 0:\n",
    "                relative_position_embedding_scalar = tmp_relative_position_embedding_scalar\n",
    "            else:\n",
    "                # すでに計算済みのヘッドがある場合は、テンソルを連結\n",
    "                relative_position_embedding_scalar = torch.cat([relative_position_embedding_scalar, tmp_relative_position_embedding_scalar], dim=0)\n",
    "\n",
    "        return relative_position_embedding_scalar\n",
    "    \n",
    "    def _outputAttention(self, query, key, value, batch_size, num_patch, seq_length, hidden_size, num_heads, check_positional_embedding, check_mask):\n",
    "        # Attentionスコアを計算する関数\n",
    "\n",
    "        # マスクや位置エンベディングの有無によって、処理を分岐\n",
    "        if check_positional_embedding:\n",
    "            seq_length1 = seq_length2 = num_patch + self.seq_length[0]\n",
    "        else:\n",
    "            if check_mask:\n",
    "                seq_length1 = seq_length2 = self.seq_length[1]\n",
    "            else:\n",
    "                seq_length1 = self.seq_length[1]\n",
    "                seq_length2 = num_patch + self.seq_length[0]\n",
    "\n",
    "        # ヘッドごとの隠れ層サイズ\n",
    "        head_hidden_size = int(hidden_size / num_heads)\n",
    "\n",
    "        # マスクマップを作成\n",
    "        mask_map = torch.tensor(np.tril(np.ones((seq_length1, seq_length2))), dtype=torch.long)\n",
    "\n",
    "        # 位置エンベディングが指定されている場合、相対位置エンベディングを計算\n",
    "        if check_positional_embedding:\n",
    "            relative_position_embedding_scalar = self._outputRelativePositionalEmbeddingScalar(query, batch_size, num_patch, seq_length, hidden_size, num_heads)\n",
    "        else:\n",
    "            relative_position_embedding_scalar = None\n",
    "\n",
    "        # 各ヘッドごとにAttentionスコアを計算\n",
    "        for id in range(num_heads):\n",
    "            head_query = self.query_module[id](query)\n",
    "            head_key = self.key_module[id](key)\n",
    "            head_value = self.value_module[id](value)\n",
    "\n",
    "            if check_positional_embedding:\n",
    "                # 位置エンベディングが指定されている場合、Attentionスコアに相対位置エンベディングを加算\n",
    "                tmp_head_attention = self.softmax(((head_query@head_key.transpose(1, 2)) / (head_hidden_size) + relative_position_embedding_scalar[id]))@head_value\n",
    "            else:\n",
    "                # 位置エンベディングが指定されていない場合\n",
    "                if check_mask:\n",
    "                    # マスクが指定されている場合、Attentionスコアにマスクを適用\n",
    "                    tmp_head_attention = self.softmax((mask_map * (head_query@head_key.transpose(1, 2)) / (head_hidden_size)))@head_value\n",
    "                else:\n",
    "                    # マスクが指定されておらず、位置エンベディングもない場合、通常のAttentionスコア計算\n",
    "                    tmp_head_attention = self.softmax((head_query@head_key.transpose(1, 2)) / (head_hidden_size))@head_value\n",
    "\n",
    "            # はじめて計算するヘッドの場合は、Attentionスコアをそのまま保持\n",
    "            if id == 0:\n",
    "                head_attention = tmp_head_attention\n",
    "            else:\n",
    "                # すでに計算済みのヘッドがある場合は、テンソルを連結\n",
    "                head_attention = torch.cat([head_attention, tmp_head_attention], dim=-1)\n",
    "\n",
    "        # 出力のAttentionスコアを返す\n",
    "        output_attention = head_attention\n",
    "\n",
    "        return output_attention\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        # フォワード関数\n",
    "\n",
    "        # Attentionスコアを計算\n",
    "        output_attention = self._outputAttention(query, key, value, self.batch_size, self.num_patch, self.seq_length, self.hidden_size, self.num_heads,\n",
    "                                                  self.check_positional_embedding, self.check_mask)\n",
    "\n",
    "        return output_attention\n",
    "    \n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, batch_size, num_patch, seq_length, hidden_size, check_encoder):\n",
    "        super().__init__()\n",
    "\n",
    "        # AddNormモジュールを構築\n",
    "        self._setupAddNormModule(batch_size, num_patch, seq_length, hidden_size, check_encoder)\n",
    "\n",
    "    def _setupAddNormModule(self, batch_size, num_patch, seq_length, hidden_size, check_encoder):\n",
    "        # エンコーダーの場合、シーケンス長はエンコーダーのシーケンス長とパッチ数の合計\n",
    "        if check_encoder:\n",
    "            seq_length = seq_length[0] + num_patch\n",
    "        else:\n",
    "            # デコーダーの場合、シーケンス長はデコーダーのシーケンス長\n",
    "            seq_length = seq_length[1]\n",
    "\n",
    "        # LayerNormモジュールを構築\n",
    "        self.layer_norm = nn.LayerNorm((batch_size, seq_length, hidden_size))\n",
    "\n",
    "    def forward(self, tokens, skipped_tokens):\n",
    "        # 入力トークンにスキップしたトークンを加算\n",
    "        tokens += skipped_tokens\n",
    "\n",
    "        # LayerNormを適用して出力トークンを生成\n",
    "        tokens = self.layer_norm(tokens)\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # FeedForwardモジュールを構築\n",
    "        self._setupFeedForwardModule(hidden_size, ffn_hidden_size)\n",
    "\n",
    "    def _setupFeedForwardModule(self, hidden_size, ffn_hidden_size):\n",
    "        # 1つ目の全結合層とReLU活性化関数\n",
    "        dense1 = nn.Linear(hidden_size, ffn_hidden_size)\n",
    "        relu1 = nn.ReLU()\n",
    "\n",
    "        # 2つ目の全結合層とReLU活性化関数\n",
    "        dense2 = nn.Linear(ffn_hidden_size, hidden_size)\n",
    "        relu2 = nn.ReLU()\n",
    "\n",
    "        # モジュールリストとして保持\n",
    "        self.feed_foward_module = nn.ModuleList([dense1, relu1, dense2, relu2])\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # フォワード関数\n",
    "\n",
    "        # モジュールリスト内の各モジュールを順に適用\n",
    "        for module in self.feed_foward_module:\n",
    "            tokens = module(tokens)\n",
    "\n",
    "        # 出力トークンを返す\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 12\n",
      "torch.Size([3, 12, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# BERTトークナイザーの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") # T5Tokenizerでは文頭トークン<s>が表示されなかったため、bertで代用\n",
    "\n",
    "# 入力となる英語のトークン化\n",
    "enocoder_input_tokens = [\"Translate English to German : Two brown and white dogs\", \n",
    "                         \"Translate English to German : The man plaing soccoer\",\n",
    "                         \"Translate English to German : The birds is flying\"]\n",
    "encoder_input_tokenize = tokenizer(enocoder_input_tokens, padding=True, return_tensors=\"pt\", return_length=True)\n",
    "encoder_input_ids = encoder_input_tokenize[\"input_ids\"][:, 1:]  # 先頭のトークン<s>を除外\n",
    "encoder_attention_mask = encoder_input_tokenize[\"attention_mask\"][:, 1:]\n",
    "encoder_attention_mask = torch.where(encoder_input_ids == 102, 0, encoder_attention_mask)  # 文末トークンを除外\n",
    "encoder_input_ids = torch.where(encoder_input_ids == 102, 0, encoder_input_ids)  # 文末トークンを除外\n",
    "\n",
    "# 出力となるドイツ語のトークン化\n",
    "decoder_input_tokens = [\"Zwei braune und weiße Hunde\",\n",
    "                       \"Der Mann spielt Fußball\",\n",
    "                       \"Die Vögel fliegen\"]\n",
    "decoder_input_tokenize = tokenizer(decoder_input_tokens, padding=True, return_tensors=\"pt\", return_length=True)\n",
    "decoder_input_ids = decoder_input_tokenize[\"input_ids\"]\n",
    "decoder_attention_mask = decoder_input_tokenize[\"attention_mask\"]\n",
    "\n",
    "encoder_max_length = encoder_input_tokenize[\"length\"][0].item() - 1\n",
    "decoder_max_length = decoder_input_tokenize[\"length\"][0].item()\n",
    "\n",
    "# 画像の生成と前処理\n",
    "images = torch.randn(3, 3, 256, 256, dtype=torch.float)\n",
    "patch_size = 16\n",
    "num_patch = int((images.shape[2] / patch_size) ** 2) * images.shape[1]\n",
    "num_embedding = torch.max(torch.concat([encoder_input_ids, decoder_input_ids], dim=-1)) + 1\n",
    "\n",
    "# SimVLMモデルの初期化と実行\n",
    "kwargs = {\n",
    "    \"batch_size\": 3,\n",
    "    \"num_patch\": num_patch,\n",
    "    \"num_embedding\": num_embedding,\n",
    "    \"seq_length\": (encoder_max_length, decoder_max_length),\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_layer\": 12,\n",
    "    \"num_heads\": 8,\n",
    "    \"ffn_hidden_size\": 3072\n",
    "}\n",
    "simvlm = SimVLM(**kwargs)\n",
    "\n",
    "# モデルの出力の形状を表示\n",
    "print(encoder_max_length, decoder_max_length)\n",
    "print(simvlm(images, encoder_input_ids, decoder_input_ids).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
